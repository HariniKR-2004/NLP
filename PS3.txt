from collections import defaultdict

def read_grammar_from_file(filename):
    grammar = defaultdict(list)
    with open(filename, 'r') as file:
        for line in file:
            if '->' in line:
                lhs, rhs = line.strip().split('->')
                lhs = lhs.strip()
                rhs_symbols = tuple(rhs.strip().split())
                grammar[lhs].append(rhs_symbols)
    return dict(grammar)

# Read the grammar
base_grammar = read_grammar_from_file("grammar.txt")

# Display it
from pprint import pprint
pprint(base_grammar)

--CYK--
from nltk.tree import Tree

grammar = {
    'S': [['NP', 'VP']],
    'NP': [['Det', 'N']],
    'VP': [['V', 'NP']],
    'Det': [['the']],
    'N': [['cat'], ['dog']],
    'V': [['chased']]
}

def cyk_parser(sentence, grammar):
    words = sentence.split()
    n = len(words)
    table = [[set() for _ in range(n)] for _ in range(n)]
    back = [[{} for _ in range(n)] for _ in range(n)]

    # Fill in length-1 entries
    for i, word in enumerate(words):
        for lhs, productions in grammar.items():
            for rhs in productions:
                if rhs == [word]:
                    table[i][i].add(lhs)
                    back[i][i][lhs] = word

    # Fill in spans > 1
    for l in range(2, n + 1):
        for i in range(n - l + 1):
            j = i + l - 1
            for k in range(i, j):
                for lhs, productions in grammar.items():
                    for rhs in productions:
                        if len(rhs) == 2:
                            B, C = rhs
                            if B in table[i][k] and C in table[k + 1][j]:
                                table[i][j].add(lhs)
                                back[i][j][lhs] = (B, i, k, C, k + 1, j)

    accepted = 'S' in table[0][n - 1]
    return accepted, table, back, words

def build_tree(back, i, j, symbol):
    if isinstance(back[i][j][symbol], str):
        return (symbol, back[i][j][symbol])
    else:
        B, i1, k, C, k1, j1 = back[i][j][symbol]
        left = build_tree(back, i1, k, B)
        right = build_tree(back, k1, j1, C)
        return (symbol, left, right)

def tuple_to_nltk_tree(tree_tuple):
    if isinstance(tree_tuple, tuple):
        label = tree_tuple[0]
        children = [tuple_to_nltk_tree(child) for child in tree_tuple[1:]]
        return Tree(label, children)
    else:
        return tree_tuple

# Run parser
sentence = 'the dog chased the cat'
result, parse_table, back_ptrs, words = cyk_parser(sentence, grammar)

print("Accepted:", result)
if result:
    parse_tree_tuple = build_tree(back_ptrs, 0, len(words)-1, 'S')
    print(parse_tree_tuple)
    tree = tuple_to_nltk_tree(parse_tree_tuple)
    print(tree)
    tree.pretty_print()


--P-CYK--
from collections import defaultdict
import pprint
from nltk.tree import Tree

# === Step 1: Define sample parse trees ===
parse_trees = [
    ('S', 
        ('NP', 'John'),
        ('VP', 
            ('V', 'called'), 
            ('NP', 
                ('NP', 'Mary'), 
                ('PP', 
                    ('P', 'from'), 
                    ('NP', 'Denver')
                )
            )
        )
    ),
    ('S', 
        ('NP', 'John'),
        ('VP', 
            ('VP', 
                ('V', 'called'), 
                ('NP', 'Mary')
            ),
            ('PP', 
                ('P', 'from'), 
                ('NP', 'Denver')
            )
        )
    )
]

# === Step 2: Extract rule counts in nested format ===
rule_counts = defaultdict(lambda: defaultdict(int))

def extract_rules(tree):
    if isinstance(tree, tuple) and len(tree) == 3:
        lhs, rhs1, rhs2 = tree
        rhs = (rhs1[0] if isinstance(rhs1, tuple) else rhs1,
               rhs2[0] if isinstance(rhs2, tuple) else rhs2)
        rule_counts[lhs][rhs] += 1
        extract_rules(rhs1)
        extract_rules(rhs2)
    elif isinstance(tree, tuple) and len(tree) == 2:
        lhs, terminal = tree
        rhs = (terminal,)
        rule_counts[lhs][rhs] += 1

for tree in parse_trees:
    extract_rules(tree)

# === Step 3: Define base grammar ===
base_grammar = {
    'S': [('NP', 'VP')],
    'VP': [('V', 'NP'), ('VP', 'PP')],
    'NP': [('NP', 'PP'), ('John',), ('Mary',), ('Denver',)],
    'PP': [('P', 'NP')],
    'V': [('called',)],
    'P': [('from',)],
}

# === Step 4: Build probabilistic grammar from base grammar ===
def build_probabilistic_grammar(base_rules, rule_counts):
    pcfg = defaultdict(dict)
    for lhs, rhss in base_rules.items():
        total = sum(rule_counts[lhs].values())
        for rhs in rhss:
            count = rule_counts[lhs][tuple(rhs)]
            prob = count / total if total > 0 else 0.0
            pcfg[lhs][tuple(rhs)] = prob
    return pcfg

pcfg = build_probabilistic_grammar(base_grammar, rule_counts)

# === Print the grammar with probabilities ===
print("üìò Grammar with Probabilities (Filtered by Base Grammar):")
pprint.pprint(dict(pcfg))

# === Step 5: CYK Algorithm ===
def cyk(words, pcfg):
    n = len(words)
    table = [[defaultdict(float) for _ in range(n)] for _ in range(n)]
    back = [[defaultdict(tuple) for _ in range(n)] for _ in range(n)]

    # Fill in terminals
    for i, word in enumerate(words):
        for lhs, rhs_dict in pcfg.items():
            for rhs, prob in rhs_dict.items():
                if len(rhs) == 1 and rhs[0] == word:
                    table[i][i][lhs] = prob
                    back[i][i][lhs] = (word,)

    # CYK dynamic programming
    for span in range(2, n + 1):
        for begin in range(n - span + 1):
            end = begin + span - 1
            for split in range(begin, end):
                for lhs, rhs_dict in pcfg.items():
                    for rhs, prob in rhs_dict.items():
                        if len(rhs) == 2:
                            B, C = rhs
                            prob_B = table[begin][split].get(B, 0)
                            prob_C = table[split+1][end].get(C, 0)
                            if prob_B > 0 and prob_C > 0:
                                prob_total = prob * prob_B * prob_C
                                if prob_total > table[begin][end].get(lhs, 0):
                                    table[begin][end][lhs] = prob_total
                                    back[begin][end][lhs] = (split, B, C)

    return table, back

# === Step 6: Build most probable tree ===
def build_tree_with_probs(table, back, words, i, j, symbol):
    prob = table[i][j][symbol]
    if i == j:
        return (symbol, prob, back[i][j][symbol][0])
    split, B, C = back[i][j][symbol]
    left = build_tree_with_probs(table, back, words, i, split, B)
    right = build_tree_with_probs(table, back, words, split + 1, j, C)
    return (symbol, prob, left, right)

def build_tree(back, words, i, j, symbol):
    if i == j:
        return (symbol, back[i][j][symbol][0])
    split, B, C = back[i][j][symbol]
    left = build_tree(back, words, i, split, B)
    right = build_tree(back, words, split + 1, j, C)
    return (symbol, left, right)

# === Step 7: Convert to NLTK Tree ===
def tuple_to_nltk_tree(tree_tuple):
    if isinstance(tree_tuple, tuple) and len(tree_tuple) == 3 and isinstance(tree_tuple[1], float):
        label = f"{tree_tuple[0]} [{tree_tuple[1]:.3f}]"
        children = [tuple_to_nltk_tree(child) for child in tree_tuple[2:]]
        return Tree(label, children)
    elif isinstance(tree_tuple, tuple):
        label = tree_tuple[0]
        children = [tuple_to_nltk_tree(child) for child in tree_tuple[1:]]
        return Tree(label, children)
    else:
        return tree_tuple

# === Step 8: Run parser on a sentence ===
sentence = ['John', 'called', 'Mary', 'from', 'Denver']
table, back = cyk(sentence, pcfg)

# === Step 9: Output result ===
start_symbol = 'S'
if start_symbol in table[0][-1]:
    print("\nüå≥ Most Probable Parse Tree:")
    tree = build_tree_with_probs(table, back, sentence, 0, len(sentence)-1, start_symbol)
    pprint.pprint(tree, width=120)
    print(f"\nTotal Probability: {table[0][-1][start_symbol]}")

    # Visual representation with probabilities
    print("\nüñºÔ∏è  Parse Tree (Visual Representation):")
    nltk_tree = tuple_to_nltk_tree(tree)
    nltk_tree.pretty_print()

    # Visual representation without probabilities
    print("\nüå≤ Parse Tree (Without Probabilities):")
    tree_no_probs = build_tree(back, sentence, 0, len(sentence)-1, start_symbol)
    nltk_tree_no_probs = tuple_to_nltk_tree(tree_no_probs)
    nltk_tree_no_probs.pretty_print()
else:
    print("No valid parse tree found.")


--Seq2Seq--
import torch
import torch.nn as nn
import torch.optim as optim
import random

def load_pairs(filepath):
    pairs = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            if '\t' in line:
                src, tgt = line.strip().split('\t')
                pairs.append((src, tgt))
    return pairs

# pairs = [
#     ("i am a student", "je suis un √©tudiant"),
#     ("he is a teacher", "il est un professeur"),
#     ("she is happy", "elle est heureuse"),
#     ("they are playing", "ils jouent"),
#     ("you are smart", "tu es intelligent")
# ]


pairs = load_pairs("data.txt")

def tokenize(sentence):
    return sentence.lower().split()

def build_vocab(sentences):
    vocab = {"<pad>": 0, "<sos>": 1, "<eos>": 2}
    idx = 3
    for sent in sentences:
        for word in tokenize(sent):
            if word not in vocab:
                vocab[word] = idx
                idx += 1
    return vocab

src_vocab = build_vocab([src for src, _ in pairs])
tgt_vocab = build_vocab([tgt for _, tgt in pairs])
inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}

print("Source Vocab:", src_vocab)
print("Target Vocab:", tgt_vocab)

def encode(sentence, vocab):
    return [vocab["<sos>"]] + [vocab[word] for word in tokenize(sentence)] + [vocab["<eos>"]]

data = [(encode(src, src_vocab), encode(tgt, tgt_vocab)) for src, tgt in pairs]

print("Encoded data:")
for src, tgt in data:
    print("SRC:", src)
    print("TGT:", tgt)

SRC_VOCAB_SIZE = len(src_vocab)
TGT_VOCAB_SIZE = len(tgt_vocab)
EMBED_SIZE = 32
HIDDEN_SIZE = 64

class Encoder(nn.Module):
    def __init__(self, input_dim, embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim)

    def forward(self, src):
        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.lstm(embedded)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(0)
        embedded = self.embedding(input)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc(output.squeeze(0))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src, tgt, teacher_forcing_ratio=0.5):
        tgt_len = tgt.shape[0]
        batch_size = 1
        tgt_vocab_size = self.decoder.fc.out_features

        outputs = torch.zeros(tgt_len, tgt_vocab_size)
        hidden, cell = self.encoder(src)

        input = tgt[0]  # <sos>
        for t in range(1, tgt_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[t] = output
            top1 = output.argmax(1)
            input = tgt[t] if random.random() < teacher_forcing_ratio else top1
        return outputs

encoder = Encoder(SRC_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)
decoder = Decoder(TGT_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)
model = Seq2Seq(encoder, decoder)

optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss(ignore_index=0)

# Training loop
for epoch in range(100):
    total_loss = 0
    for src, tgt in data:
        src_tensor = torch.tensor(src).unsqueeze(1)  # (seq_len, 1)
        tgt_tensor = torch.tensor(tgt).unsqueeze(1)

        optimizer.zero_grad()
        output = model(src_tensor, tgt_tensor)

        output_dim = output.shape[-1]
        output = output[1:].view(-1, output_dim)
        tgt_tensor = tgt_tensor[1:].view(-1)

        loss = criterion(output, tgt_tensor)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

def translate(model, sentence, max_len=10):
    model.eval()
    tokens = encode(sentence, src_vocab)
    src_tensor = torch.tensor(tokens).unsqueeze(1)

    hidden, cell = model.encoder(src_tensor)
    input = torch.tensor([tgt_vocab["<sos>"]])

    result = []
    for _ in range(max_len):
        output, hidden, cell = model.decoder(input, hidden, cell)
        top1 = output.argmax(1).item()
        if top1 == tgt_vocab["<eos>"]:
            break
        result.append(inv_tgt_vocab[top1])
        input = torch.tensor([top1])

    return ' '.join(result)

print("\nTranslation Examples:")
print("Input: 'i am a student'")
print("Output:", translate(model, "i am a student"))

print("Input: 'you are smart'")
print("Output:", translate(model, "you are smart"))



--Transformers(MT)--
import pandas as pd
from datasets import Dataset
from transformers import (
    MarianTokenizer,
    MarianMTModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)

# Config
model_name = "Helsinki-NLP/opus-mt-en-fr"
batch_size = 4
epochs = 3

# Load dataset (CSV with 'source' and 'target' columns)
df = pd.read_csv("/content/drive/MyDrive/SEMESTER 8/Natural Language Processing/Data/translation_data.csv")
dataset = Dataset.from_pandas(df)

# Load model and tokenizer
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# Preprocessing
def preprocess(example):
    model_inputs = tokenizer(example["source"], max_length=128, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./tmp",
    per_device_train_batch_size=batch_size,
    num_train_epochs=epochs,
    report_to="none"
)

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Fine-tune the model (in-memory)
trainer.train()

# Use directly without saving
def translate(text):
    encoded = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    generated = model.generate(**encoded)
    return tokenizer.decode(generated[0], skip_special_tokens=True)

# Example usage
print(translate("How are you?"))



--Transformers(TS)--
import pandas as pd
from datasets import Dataset
from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments, Trainer

# Load small dataset sample
data = pd.read_csv("/content/drive/MyDrive/SEMESTER 8/Natural Language Processing/Data/summarization_data.csv")
sample_size = min(100, len(data))
data = data.sample(n=sample_size, random_state=42)
dataset = Dataset.from_pandas(data)

# Load model and tokenizer
model_name = "sshleifer/distilbart-cnn-12-6"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Preprocessing function
def preprocess_function(examples):
    inputs = tokenizer(examples["text"], max_length=512, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=128, truncation=True, padding="max_length")
    inputs["labels"] = labels["input_ids"]
    return inputs

# Tokenize dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Minimal training arguments
training_args = TrainingArguments(
    output_dir="./tmp",  # Required even if not saving
    per_device_train_batch_size=2,
    num_train_epochs=1,
    report_to="none"
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

# Train (quick)
trainer.train()

# Example usage without saving
def summarize(text):
    inputs = tokenizer([text], return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=128, min_length=30, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Try the model
example = """Machine learning is a method of data analysis that automates analytical model building.
It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention."""
print(summarize(example))



--Transformers(SA)--
import pandas as pd
from datasets import Dataset
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)

# Load and prepare dataset
df = pd.read_csv("/content/drive/MyDrive/SEMESTER 8/Natural Language Processing/Data/sentiment_data.csv")  # Replace with your file path
label_map = {"negative": 0, "positive": 1}
df["label"] = df["label"].map(label_map)

dataset = Dataset.from_pandas(df)

# Split into train and test
dataset = dataset.train_test_split(test_size=0.1)

# Load tokenizer and model
model_name = "distilbert-base-uncased"
tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Preprocessing
def preprocess(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

tokenized = dataset.map(preprocess, batched=True)

# Minimal training arguments
training_args = TrainingArguments(
    output_dir="./tmp",  # Required even if not saving
    per_device_train_batch_size=4,
    num_train_epochs=3,
    report_to="none"
)

# Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator
)

# Train the model (quick)
trainer.train()

# Example usage: Sentiment prediction
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=256)
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_class = torch.argmax(logits, dim=-1).item()
    return "positive" if predicted_class == 1 else "negative"

# Test the prediction
example = "I dont love this new phone!"
print(predict_sentiment(example))
